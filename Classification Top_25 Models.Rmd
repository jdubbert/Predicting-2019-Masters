---
title: "PGA Classification: Predicting Masters Tournament Top 25 Finshes"
author: "Jacob Dubbert"
date: "3/18/2019"
output:
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### PROJECT DESCRIPTION
The purpose of this project is to identify the best machine learning algorithm in predicting the Top 25 players that finish in the Master's Tournament. It is a binary classification problem with the goal of predicting the variable `top_25`. The following models will be compared:

1. Logistic regression
2. Linear discriminant analysis
3. Quadratic discriminant analysis
4. Boosted algorithm
5. Random Forest
5. Bagged algorithm
6. Support vector machine
7. Neural Net
8. Lasso Regression
9. Polynomial regression

```{r libs, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(tree)
library(nnet)
library(dplyr)
library(car)
library(plotROC)
library(kernlab)
library(MASS)
library(randomForest)
library(kernlab) 
library(naivebayes)
library(pROC)
library(ROCR)
library(verification)
```

### Data Import & Feature Engineering
The data used in this analysis was scraped from the PGA Tour website. The data scraper can be found in the "PGA_Scrape_Data.Rmd"

To add to the analysis, I decided to do some feature engineering by creating new variables. 

1. `m_cut` is a dependent variable that is a 0 if a player missed the cut and a 1 if a player made the cut; this will be used in the classification models. 
2. `top_25` is a dependent variable that is coded 1 if the player placed in the top 25 and 0 if they didnt; this will be used in the classifcation models. 
3. `new-score` is a dependent variable that adds 160 to a player's final score if the player didnt make the cut - this is to allow for a similar number to players that did make the cut for regression modeling. This solves the problem of a player that missed the cut only having total strokes for two days, as opposed to 4 days like the players that made the cut. 
4. `p_masters` is an independent variable which is the Masters finishing position for that player in the previous year.

```{r}
df2 <- read.csv("df2.csv", stringsAsFactors = FALSE)
```
## Convert variables to numeric and factors
```{r}
df2$wgr <- as.numeric(df2$wgr)
df2$rounds <- as.numeric(df2$rounds)
df2$wins <- as.numeric(df2$wins)
df2$top_10 <- as.numeric(df2$top_10)
df2$ranking <- as.numeric(df2$ranking)
df2$masters_finish <- as.numeric(df2$masters_finish)
df2$m_cut <- as.factor(df2$m_cut)
df2$top_25 <- as.factor(df2$top_25)
df2$m_play <- as.factor(df2$m_play)
df2 <- df2[,-1]
```

### EXPLORATORY DATA ANALYSIS
There are a total of 742 observations covering the years 2005-2017. The dependent variable is `top_25` and there are 23 independent variables. 
```{r}
summary(df2)
```
```{r}
df2$top_25 <- as.factor(df2$top_25)
p1<-ggplot(data = df2,aes(x = top_25,y = wgr,fill=top_25))+geom_boxplot()
p2<-ggplot(data = df2,aes(x = top_25,y = ranking,fill=top_25))+geom_boxplot()
p3<-ggplot(data = df2,aes(x = top_25,y = top_10,fill=top_25))+geom_boxplot()
p4<-ggplot(data = df2,aes(x = top_25,y = wins,fill=top_25))+geom_boxplot()
p5<-ggplot(data = df2,aes(x = top_25,y = score_average,fill=top_25))+geom_boxplot()
p6<-ggplot(data = df2,aes(x = top_25,y = rounds,fill=top_25))+geom_boxplot()
p7<-ggplot(data = df2,aes(x = top_25,y = bounce_back,fill=top_25))+geom_boxplot()
p8<-ggplot(data = df2,aes(x = top_25,y = driving_accuracy,fill=top_25))+geom_boxplot()
p9<-ggplot(data = df2,aes(x = top_25,y = driving_distance,fill=top_25))+geom_boxplot()
p10<-ggplot(data = df2,aes(x = top_25,y = par5_SA,fill=top_25))+geom_boxplot()
p11<-ggplot(data = df2,aes(x = top_25,y = gir,fill=top_25))+geom_boxplot()
p12<-ggplot(data = df2,aes(x = top_25,y = hole_proximity,fill=top_25))+geom_boxplot()
p13<-ggplot(data = df2,aes(x = top_25,y = putts_round,fill=top_25))+geom_boxplot()
p14<-ggplot(data = df2,aes(x = top_25,y = scramble,fill=top_25))+geom_boxplot()
p15<-ggplot(data = df2,aes(x = top_25,y = sg_putt,fill=top_25))+geom_boxplot()
p16<-ggplot(data = df2,aes(x = top_25,y = sg_t2g,fill=top_25))+geom_boxplot()
p17<-ggplot(data = df2,aes(x = top_25,y = sg_total,fill=top_25))+geom_boxplot()
p18<-ggplot(data = df2,aes(x = top_25,y = points_gained,fill=top_25))+geom_boxplot()

grid.arrange(p1,p2,p3,p4,p5,p6,nrow=3)
```

```{r}
grid.arrange(p7,p8,p9,p10,p11,p12,nrow=3)
```

```{r}
grid.arrange(p13,p14,p15,p16,p17,p18,nrow=3)
```


### SPLIT DATA INTO TESTING AND TRAINING DATASETS
I divided the PGA data into a "training" set and a "testing" set, taking the data from 2005-2017. The training set is used to estimate the parameters of the model, and then the testing set is used to evaluate the predictions of the model. The models are evaluated using Accuracy and AOC, which in this context is defined as the difference between our predicted `top_25` and the observed `top_25. Cross validation involves repeating this process several times (i.e. dividing your sample into training and testing sets) and averaging the model's performance on the testing sets. This repetitive process is again done to avoid overfitting. The model that performs the best in the cross validation exercise should be the one that generalizes the best to new data. Once the best model is found, we will predict for the 2019 Masters using the data from 2018.

```{r}
set.seed(1234)

inTraining <- createDataPartition(df2$score_average, p = .75, list = F)
training <- df2[inTraining,]
testing <- df2[-inTraining,]
```

```{r}
dim(training)
```
```{r}
dim(testing)
```

### MODELING

## Logistic Regression
All independent variables
```{r}
glm_fits <- glm(top_25 ~ .-year-new_score-total_score-masters_finish-m_cut, 
                   family = "binomial",
                   data = training)
summary(glm_fits)
glm_preds <- round(predict(glm_fits, testing, type= "response"))
confusionMatrix(table(glm_preds,testing$top_25), positive = "1")
```

Refit model with important predictors chosen with stepwise selection.
```{r}
glm_fits_1 <- glm(top_25 ~ .-year-new_score-total_score-masters_finish-m_cut-sg_total-scramble-putts_round-rounds-par5_SA-bounce_back-top_10-score_average-ranking-driving_accuracy-driving_distance-sg_putt-gir-hole_proximity-wins-m_play, 
                   family = "binomial",
                   data = training)
summary(glm_fits_1)
glm_preds_1 <- predict(glm_fits_1, testing, type= "response")
a <- confusionMatrix(table(glm_preds_1,testing$top_25), positive = "1")
logistic <- a$overall[1]
logistic
```
ROC
```{r}
roc.plot(x=testing$top_25=="1", pred=glm_preds_1)$roc.vol
```

## Linear Discriminant Analysis
```{r}
lda_fits <- lda(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score, data=training)
lda_fits
lda_preds <- predict(lda_fits, testing)
confusionMatrix(table(lda_preds$class, testing$top_25), positive = "1")
```

Refit model with same predictors used in GLM
```{r}
set.seed(1234)
lda_fits_1 <- lda(top_25 ~ .-year-new_score-total_score-masters_finish-m_cut-sg_total-scramble-putts_round-rounds-par5_SA-bounce_back-top_10-score_average-ranking-driving_accuracy-driving_distance-sg_putt-gir-hole_proximity-wins-m_play,
                   data = training)
lda_fits_1
lda_preds_1 <- predict(lda_fits_1, testing)
c <-confusionMatrix(table(lda_preds_1$class, testing$top_25), positive = "1")
linear_discriminant <- c$overall[1]
linear_discriminant
```
ROC Curve
```{r}
roc.plot(x=testing$top_25=="1", pred=lda_preds_1$posterior[,2])$roc.vol
```

## Quadratic Discriminant Analysis
```{r}
qda_fits <- qda(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score, data=training)
qda_fits
qda_preds <- predict(qda_fits, testing)
confusionMatrix(table(qda_preds$class, testing$top_25), positive = "1")
```

Refit with important predictors 
```{r}
set.seed(1234)
qda_fits_1 <- qda(top_25 ~ .-year-new_score-total_score-masters_finish-m_cut-sg_total-scramble-putts_round-rounds-par5_SA-bounce_back-top_10-score_average-ranking-driving_accuracy-driving_distance-sg_putt-gir-hole_proximity-wins-m_play,
                   data = training)
qda_fits_1
qda_preds_1 <- predict(qda_fits_1, testing)
e <-confusionMatrix(table(qda_preds_1$class, testing$top_25), positive = "1")
quadratic_discriminant <- e$overall[1]
quadratic_discriminant
```

ROC Curve
```{r}
roc.plot(x=testing$top_25=="1", pred = qda_preds_1$posterior[,2])$roc.vol
```


## Polynomial Regression
```{r}
glm_poly <- glm(top_25 ~ poly(wgr, 2)+ poly(sg_t2g, 2) + poly(points_gained, 2), data = training, family = "binomial")
tidy(glm_poly)
poly_preds<- predict(glm_poly, testing, type="response")
p <-confusionMatrix(table(poly_preds, testing$top_25), positive = "1")
polynomial <- p$overall[1]
polynomial
```
ROC
```{r}
roc.plot(x=testing$top_25=="1", pred = poly_preds)$roc.vol
```

## Lasso Regression
Create training and testing datasets
```{r}
lambdas <- 10^seq(-2, 3, len = 100)
x <- model.matrix(top_25 ~ ., df2)[, -c(1,2,4,5,24,23)]
y <- df2$top_25

x_train <- x[inTraining, ]
x_test  <- x[-inTraining, ]

y_train <- y[inTraining]
y_test <- y[-inTraining]
```

```{r}
lasso_mod <- glmnet(x_train, y_train, alpha = 1, family = "binomial")
plot(lasso_mod, xvar = "lambda")
lasso_mod
coef(lasso_mod)[,10]
```
Cross validation to find best value of lambda
```{r}
set.seed(1234)
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, lambda = lambdas, family="binomial") 
plot(lasso_cv) 
lasso_cv$lambda.1se # this is the value of lamda that you want to use
bestlam <- lasso_cv$lambda.1se
bestlam
coef(lasso_cv, s=bestlam)
```
Test 
```{r}
lasso_pred <- predict(lasso_mod, newx = x_test, s=bestlam, type="response")

c_lasso_test_df <- as.data.frame(y_test) %>%
  mutate(preds=ifelse(as.numeric(lasso_pred)>=.5,"1","0"))
lass<-confusionMatrix(table(c_lasso_test_df$y_test,c_lasso_test_df$preds), positive = "1")
lasso <- lass$overall[1]
lasso
```
ROC
```{r}
roc.plot(x=testing$top_25=="1", pred=lasso_pred)$roc.vol
```

## Bagged model
```{r}
set.seed(1234)
c_bag_pga <- randomForest(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score,
                          data = training, 
                          mtry = 18,
                          ntree=500)
c_bag_pga
```
Test MSE
```{r}
c_bag_test_preds <- predict(c_bag_pga, newdata = testing)
c_bag_test_df <- testing %>%
  mutate(preds=c_bag_test_preds)
confusionMatrix(table(c_bag_test_df$top_25,c_bag_test_df$preds), positive = "1")
```
WITH LASSO PREDICTORS
```{r}
set.seed(1234)
c_bag_pga2 <- randomForest(top_25 ~ wgr+score_average+sg_total+points_gained,
                          data = training, 
                          mtry = 4,
                          ntree=600)
c_bag_pga2
c_bag_test_preds2 <- predict(c_bag_pga2, newdata = testing, type="response")
c_bag_test_df2 <- testing %>%
  mutate(preds=c_bag_test_preds2)
bgg<-confusionMatrix(table(c_bag_test_df2$top_25,c_bag_test_df2$preds), positive = "1")
bagged <- bgg$overall[1]
bagged
```
ROC
```{r}
roc.plot(x=training$top_25=="1", pred=c_bag_test_preds2)$roc.vol
```

## Random Forest
```{r}
set.seed(1234)
c_rf_pga <- randomForest(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt, 
                          data = training,
                          mtry = 1)
c_rf_pga
```

Test MSE
```{r}
c_rf_test_preds <- predict(c_rf_pga, newdata = testing)
c_rf_test_df <- testing %>%
  mutate(preds=c_rf_test_preds)
confusionMatrix(table(c_rf_test_df$top_25,c_rf_test_df$preds), positive = "1")
```

Determine best number of predictors with cross-validation
```{r}
set.seed(1234)

c_rf_cv <- train(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                      data = training,
                      method = "rf",
                      ntree = 100,
                      importance = T,
                      tuneGrid = data.frame(mtry = 1:13))
c_rf_cv
plot(c_rf_cv)
```

Refit model with mtry=1
```{r}
set.seed(1234)
c_rf_pga_1 <- randomForest(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt, 
                          data = training,
                          mtry = 1,
                          ntree=500)
c_rf_pga_1
```

Test MSE
```{r}
c_rf1_test_preds <- predict(c_rf_pga_1, newdata = testing,type="prob")
c_rf1_test_df <- testing %>%
  mutate(preds=c_rf1_test_preds)
g <-confusionMatrix(table(c_rf1_test_df$top_25,c_rf1_test_df$preds), positive = "1")
random_forest <- g$overall[1]
random_forest
```
ROC
```{r}
roc.plot(x= testing$top_25=="1" , pred= c_rf1_test_preds[,2])$roc.vol
```

Identify important variables
```{r}
c_rf_imp <- varImp(c_rf_cv)$importance 
rn <- row.names(c_rf_imp)
imp_df <- data_frame(variable = rn, 
                     importance = c_rf_imp$`1`) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```

## Boosted Model
Find best parameters
```{r}
set.seed(1234)
grid <- expand.grid(interaction.depth = c(1, 3), 
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10) 
trainControl <- trainControl(method = "cv", number = 10)
gbm_pga <- caret::train(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt, 
                    data = training,
                    method="gbm",
                    trControl = trainControl,
                    tuneGrid=grid)
gbm_pga
```
Train model
```{r}
gbm_train <- gbm(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                 data = training,
                 n.trees = 200,
                 interaction.depth = 1,
                 shrinkage = 0.01,
                 n.minobsinnode = 10)
gbm_train
```
Identify most important predictors
```{r}
imp <- varImp(gbm_pga)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```
Test model
```{r}
gbm_preds <- predict(gbm_pga, newdata = testing, n.trees=200, type="prob")
gbm_test_df <- testing %>%
  mutate(preds = gbm_preds)
gg<-confusionMatrix(table(gbm_test_df$top_25,gbm_test_df$preds), positive = "1")
boosted <- gg$overall[1]
boosted
```
ROC
```{r}
roc.plot(x=testing$top_25=="1", pred=gbm_preds[,2])$roc.vol
```

## Support Vector Machine
```{r}
set.seed(1234)
pga_svc <- ksvm(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                  data = training,
                  type = "C-svc", kernel = 'vanilladot', prob.model = TRUE)
fits_svc <- predict(pga_svc, newdata = testing, type = "probabilities")
svc_test_df <- testing %>%
  mutate(preds = fits_svc[,2])
confusionMatrix(table(predict(pga_svc, newdata = testing), testing$top_25), positive = "1")
```
Cross validation to find the "best" value of $C$.
```{r}
fit_control <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 3)
svc_train <- caret::train(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                     data = training,
                     method = "svmLinear",
                     trControl = fit_control,
                     tuneGrid = data.frame(C = 1:10))
svc_train
plot(svc_train)
confusionMatrix(svc_train)
```
Test Accuracy
```{r}
svc<-confusionMatrix(table(predict(svc_train, newdata = testing), 
                      testing$top_25), positive = "1")
support_vector <- svc$overall[1]
support_vector
```
ROC
```{r}
set.seed(1234)
pga_svc2 <- ksvm(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                  data = training,
                  type = "C-svc", kernel = 'vanilladot', prob.model = TRUE, C=4)
svc_predict <- predict(pga_svc2, newdata=testing, type="prob")
roc.plot(x=testing$top_25=="1", pred=svc_predict[,2])$roc.vol
```

With polynomial kernal
```{r}
set.seed(1234)
pga_svc_poly <- ksvm(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                  data = training,
                  type = "C-svc", kernel = 'polydot', kpar = list(degree = 2, scale = .1),
                     C = 1, prob.model = T)
confusionMatrix(table(predict(pga_svc_poly, newdata = testing), testing$top_25), positive = "1")
```
Tune parameters
```{r}
pga_train_poly <- caret::train(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                          data = training,
                          method = "svmPoly",
                          trControl = fit_control,
                          tuneGrid = expand.grid(degree = 2:4,
                                                 scale = c(.001, .01, .1), 
                                                 C = 2:8))
pga_train_poly
poly <- confusionMatrix(table(predict(pga_train_poly, newdata = testing), 
                      testing$top_25), 
                positive = "1")
svm_poly <- poly$overall[1]
svm_poly
```
ROC
```{r}
set.seed(1234)
pga_svc_poly2 <- ksvm(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                  data = training,
                  type = "C-svc", kernel = 'polydot', kpar = list(degree = 4, scale = .01),
                     C = 5, prob.model = T)
svc_predict_poly <- predict(pga_svc_poly2, newdata=testing, type="prob")
roc.plot(x=testing$top_25=="1", pred=svc_predict_poly[,2])$roc.vol
```

With radial basis kernel
```{r}
pga_svm_rad <- ksvm(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                      data=training,
                      type = "C-svc", kernel = 'rbfdot', 
                      kpar = list(sigma = .1),
                      prob.model = T)
confusionMatrix(table(predict(pga_svm_rad, newdata = testing), 
                      testing$top_25), positive = "1")
```

## Neural Net
```{r}
set.seed(1234)
c_nn_pga <- nnet(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                  data = training,
                  size = 2,
                  decay = 0.01,
                  maxit = 1000)
```

Test Accuracy
```{r}
tmp <- predict(c_nn_pga, newdata = testing, type = "class") 
confusionMatrix(as.factor(tmp), testing$top_25)
```

Find optimal values using cross-validation
```{r}
set.seed(1234)
tune_grid <- expand.grid(size = 1:3,
                         decay = c(.1, .5, 1))
fit_control <- trainControl(method = "cv",
                            number = 5,
                            verboseIter = TRUE)
c_nn_pga_train <- caret::train(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                         data = training,
                         method = "nnet",
                         tuneGrid = tune_grid,
                         trControl = fit_control,
                         maxit = 500,
                         trace = FALSE,
                         verbose = TRUE)
```

```{r}
c_nn_pga_train
```

Refit model
```{r}
set.seed(1234)
c_nn_new <- nnet(top_25 ~ . -year-masters_finish-m_cut-total_score-new_score-bounce_back-sg_putt,
                  data = training,
                  size = 3,
                  decay = 1,
                  maxit = 1000)
```

Test Accuracy
```{r}
tmp2 <- predict(c_nn_new, newdata = testing) 
nnn <-confusionMatrix(as.factor(tmp2), testing$top_25)
neural_net <- nnn$overall[1]
neural_net
```
ROC
```{r}
roc.plot(x=testing$top_25=="1", pred=tmp2)$roc.vol
```

### MODEL COMPARISON
Below the Accuracy and AUC of the different models are compared to choose the best model. We will use the AUC to choose the best model. 
```{r}
guess <- 464/742
model_compare <- tibble(guess,logistic, linear_discriminant, quadratic_discriminant, lasso,random_forest, polynomial, bagged, boosted, svm_poly, support_vector, neural_net)
model_compare <- model_compare %>% gather(model, Accuracy) %>% arrange(desc(Accuracy))
model_compare
```

Comparison of AUC for test data
```{r}
roc_plot<-roc.plot(x=testing$top_25=="1",pred=cbind(glm_preds_1,lda_preds_1$posterior[,2],
                                      qda_preds_1$posterior[,2],poly_preds,c_rf1_test_preds[,2],gbm_preds[,2],
                                      svc_predict[,2],svc_predict_poly[,2],tmp2,lasso_pred),legend = T,
         leg.text = c("Logistic","Linear Discriminant","Quadradic Discriminant",
                      "Polynomial","Random Forest","GBM","SVM","SVM_poly","Neural Net","Lasso"), thresholds = thresh)$roc.vol
roc_plot
```

```{r}
roc_plot[,1:2] %>% arrange(desc(Area)) %>% mutate(model=c("Linear Discriminant","Logistic","Quadradic Discriminant","Neural Net","Polynomial","Lasso","GBM","SVM","SVM Polynomial","Random Forest"))
```

### PREDICT 2019 MASTERS TOURNAMENT TOP 25
Here the best model, the Quadratic Discriminant, is used to predict the Top 25 of the 2019 Masters Tournament. The data from 2018 is used in the model. 

Data
```{r}
data$year<- as.numeric(data$year)
new_data <- data %>% filter(year>2016)
new_data <- new_data %>% arrange(player_name)
new_data<- new_data %>% group_by(player_name) %>% mutate(mast_last=lag(masters_finish))
new_data <- mutate(new_data, m_play= if_else(as.numeric(mast_last)=="NA",as.numeric(0),1))
new_data$m_play[is.na(new_data$m_play)] <- 0
new_data$m_play <- as.factor(new_data$m_play)
new_data <- new_data[,-c(1)]
new_data <- new_data[!is.na(new_data$score_average),]
new_data$wgr <- as.numeric(new_data$wgr)
new_data$rounds <- as.numeric(new_data$rounds)
new_data$wins <- as.numeric(new_data$wins)
new_data$top_10 <- as.numeric(new_data$top_10)
new_data$ranking <- as.numeric(new_data$ranking)
df_2018 <- new_data %>% filter(year==2018)
df_2018 <- df_2018[,-3]
wgr_2018 <- read.csv("wgr_2019.csv")
wgr_2018<- wgr_2018[,-1]
df_2018 <- left_join(df_2018, wgr_2018, by=c("player_name", "year"))
df_2018$wgr[is.na(df_2018$wgr)] <- df_2018$ranking[is.na(df_2018$wgr)]
df_2018$points_gained[is.na(df_2018$points_gained)] <-0
df_2018$m_cut <- if_else(df_2018$masters_finish==99,0,1)
df_2018$m_cut <- as.factor(df_2018$m_cut)
df_2018$top_25 <- if_else(df_2018$masters_finish<=25,1,0)
df_2018$top_25 <- as.factor(df_2018$top_25)
df_2018 <- mutate(df_2018, new_score = if_else(as.numeric(masters_finish) == 99, total_score + 160, as.numeric(total_score)))
rownames(df_2018)<- df_2018$player_name
df_2018<- df_2018 %>% dplyr::select(-player_name)
df_2018<- as.data.frame(df_2018)
write.csv(df_2018, "data_2018")
```

Prediction and analysis of Accuracy
```{r}
qda_2019 <- predict(qda_fits_1, df_2018)
options(scipen=999)
predictions <- df_2018 %>% mutate(preds=qda_2019$class,
                                  probs=qda_2019$posterior[,2])
```

Top 25 Predictions
```{r}
final <- predictions %>% filter(preds=="1") %>% arrange(desc(probs))
final$Top_25 <- "Yes"

DT::datatable(final[,c(1,30,29)], rownames=FALSE,options = list(
  pageLength=25
))
```

